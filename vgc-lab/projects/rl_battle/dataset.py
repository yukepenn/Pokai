"""Dataset for in-battle move choice behavior cloning."""

from __future__ import annotations

import json
from dataclasses import dataclass, field
from typing import Dict, List, Optional

import numpy as np
import torch
from torch.utils.data import Dataset

from vgc_lab import get_paths, iter_trajectories, encode_state_from_request
from vgc_lab.core import SanitizeReason


@dataclass
class BattleStepDatasetConfig:
    """Configuration for BattleStepDataset."""

    format_id: str = "gen9vgc2026regf"
    # Optional: minimum number of examples, just for sanity logging
    min_examples: int = 100
    # Policy-based filtering (both None = no filtering, preserves existing behavior)
    include_battle_policy_ids: Optional[List[str]] = None
    exclude_battle_policy_ids: Optional[List[str]] = None
    # Sanitize reason filtering: if None, include all steps regardless of sanitize_reason.
    # If provided, only steps with sanitize_reason in this list are included.
    # Default: ["ok", "fixed_pass"] - only include clean steps and steps where we fixed a "pass"
    train_allowed_sanitize_reasons: Optional[List[SanitizeReason]] = field(
        default_factory=lambda: ["ok", "fixed_pass"]
    )


def is_valid_battle_choice(choice: str) -> bool:
    """
    Return True if this step.choice should be used for BC training.

    We keep this intentionally simple:
    - Reject empty / whitespace-only.
    - Reject 'default' (case-insensitive).
    - Reject choices that start with 'team ' (team preview choices).
    - Reject choices that start with '/' or contain obvious control commands.
    - Optionally reject strings containing 'forfeit' or 'resign'.
    """
    if not choice or not choice.strip():
        return False
    s = choice.strip().lower()
    if s == "default":
        return False
    if s.startswith("team "):
        return False
    if s.startswith("/"):
        return False
    if "forfeit" in s or "resign" in s:
        return False
    return True


def encode_battle_state_to_vec(state_dict: dict, *, vec_dim: int = 256) -> np.ndarray:
    """
    Convert a state_dict (as returned by encode_state_from_request) into a
    fixed-length numeric vector of shape (vec_dim,).

    This is intentionally simple and placeholder-like: it JSON-serializes
    the dict and maps characters to floats in [0, 1]. The goal is to have
    a stable, reproducible representation that can be reused by both the
    dataset and the policy.
    """
    state_json = json.dumps(state_dict, sort_keys=True)
    # Map characters to floats; truncate or pad to vec_dim
    vals: List[float] = [float(ord(c)) / 255.0 for c in state_json[:vec_dim]]
    if len(vals) < vec_dim:
        vals.extend([0.0] * (vec_dim - len(vals)))
    return np.asarray(vals[:vec_dim], dtype=np.float32)


def parse_move_choice(choice: str) -> Optional[int]:
    """
    Parse a Showdown choice string for a simple 'move N' choice.

    Returns:
        action_index in [0, 3] if choice is of the form 'move N' with 1 <= N <= 4,
        otherwise None.
    """
    choice = choice.strip()
    # Example formats from Showdown are typically 'move 1', 'move 2', etc.
    if not choice.startswith("move "):
        return None
    try:
        parts = choice.split()
        if len(parts) < 2:
            return None
        move_idx = int(parts[1])
    except ValueError:
        return None
    if 1 <= move_idx <= 4:
        return move_idx - 1
    return None


class BattleStepDataset(Dataset[Dict[str, torch.Tensor]]):
    """PyTorch Dataset for in-battle joint action choices."""

    def __init__(self, cfg: Optional[BattleStepDatasetConfig] = None) -> None:
        """Initialize dataset from trajectories.

        Args:
            cfg: Configuration. If None, uses default BattleStepDatasetConfig().
        """
        if cfg is None:
            cfg = BattleStepDatasetConfig()

        self._states: List[np.ndarray] = []
        self._actions: List[int] = []
        self.action_counts: Dict[int, int] = {}
        self._num_features: int = 0
        self._choice_to_id: Dict[str, int] = {}
        self.id_to_choice: List[str] = []
        self._num_actions: int = 0
        self._num_invalid_choices: int = 0
        self._num_total_steps: int = 0
        self._policy_counts: Dict[str, int] = {}
        self._outcomes: List[int] = []  # +1 = win, 0 = draw/unknown, -1 = loss
        self._outcome_counts: Dict[int, int] = {}
        self._num_filtered_by_sanitize_reason: int = 0

        paths = get_paths()
        num_trajectories = 0

        def _get_battle_policy_id_for_side(traj, side_id: str) -> str:
            """
            Best-effort extraction of battle policy id for this side.

            For now:
              - Default to "node_random_v1" (since battles are generated by a logging random AI).
              - If traj.meta exists and is a dict, try:
                  - "battle_policy_id_p1" / "battle_policy_id_p2" (side-specific)
                  - "battle_policy_id"      (shared for both sides)
            """
            default = "node_random_v1"
            meta = getattr(traj, "meta", None)
            if not isinstance(meta, dict):
                return default
            key = "battle_policy_id_p1" if side_id == "p1" else "battle_policy_id_p2"
            if key in meta and meta[key]:
                return str(meta[key])
            if "battle_policy_id" in meta and meta["battle_policy_id"]:
                return str(meta["battle_policy_id"])
            return default

        def _get_side_outcome(traj, side_id: str) -> int:
            """
            Return +1 if this side won, -1 if this side lost,
            0 if draw/unknown.
            """
            winner_side = getattr(traj, "winner_side", None)
            if winner_side not in ("p1", "p2"):
                return 0
            if winner_side == side_id:
                return 1
            return -1

        for traj in iter_trajectories(paths):
            if traj.format_id != cfg.format_id:
                continue

            num_trajectories += 1

            # For both sides "p1" and "p2"
            for side_id in ("p1", "p2"):
                policy_id_str = _get_battle_policy_id_for_side(traj, side_id)

                # Apply filtering based on config
                if cfg.include_battle_policy_ids is not None and policy_id_str not in cfg.include_battle_policy_ids:
                    # Skip this side entirely
                    continue
                if cfg.exclude_battle_policy_ids is not None and policy_id_str in cfg.exclude_battle_policy_ids:
                    # Skip this side entirely
                    continue

                side_outcome = _get_side_outcome(traj, side_id)

                steps = traj.steps_p1 if side_id == "p1" else traj.steps_p2
                for step in steps:
                    self._num_total_steps += 1
                    choice_str = step.choice
                    if not is_valid_battle_choice(choice_str):
                        self._num_invalid_choices += 1
                        continue

                    # Filter by sanitize_reason if configured
                    if cfg.train_allowed_sanitize_reasons is not None:
                        step_reason = step.sanitize_reason
                        # If sanitize_reason is None, treat it as a filtered step
                        if step_reason not in cfg.train_allowed_sanitize_reasons:
                            self._num_filtered_by_sanitize_reason += 1
                            continue

                    choice_str = choice_str.strip()

                    # Encode state from the request
                    state_dict = encode_state_from_request(step.request)
                    state_arr = encode_battle_state_to_vec(state_dict, vec_dim=256)

                    # Map choice_str to an integer ID
                    if choice_str not in self._choice_to_id:
                        new_id = len(self.id_to_choice)
                        self._choice_to_id[choice_str] = new_id
                        self.id_to_choice.append(choice_str)
                    action_id = self._choice_to_id[choice_str]

                    self._states.append(state_arr)
                    self._actions.append(action_id)
                    self.action_counts[action_id] = self.action_counts.get(action_id, 0) + 1
                    # Track policy counts
                    self._policy_counts[policy_id_str] = self._policy_counts.get(policy_id_str, 0) + 1
                    # Outcome tracking
                    self._outcomes.append(side_outcome)
                    self._outcome_counts[side_outcome] = self._outcome_counts.get(side_outcome, 0) + 1

        # Set num_features from first example if available
        if self._states:
            self._num_features = self._states[0].shape[-1] if len(self._states[0].shape) > 0 else len(self._states[0])
        else:
            self._num_features = 0

        self._num_actions = len(self.id_to_choice)

        # Print summary
        print(
            f"BattleStepDataset: {num_trajectories} trajectories, "
            f"{len(self._states)} examples"
        )
        if self._num_filtered_by_sanitize_reason > 0:
            print(
                f"Filtered {self._num_filtered_by_sanitize_reason} steps by sanitize_reason "
                f"(allowed: {cfg.train_allowed_sanitize_reasons})"
            )
        print(f"Num unique joint actions: {self._num_actions}")
        print("Action distribution (by id):", self.action_counts)
        if self._policy_counts:
            print("BattleStepDataset: policy_id -> example count:")
            for pid, count in sorted(self._policy_counts.items(), key=lambda kv: kv[0]):
                print(f"  {pid!r}: {count}")

        if len(self._states) < cfg.min_examples:
            print(
                f"WARNING: Dataset has {len(self._states)} examples, "
                f"less than min_examples={cfg.min_examples}"
            )

    def __len__(self) -> int:
        return len(self._states)

    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        state_tensor = torch.from_numpy(self._states[idx])
        action_tensor = torch.tensor(self._actions[idx], dtype=torch.long)

        sample: Dict[str, torch.Tensor] = {
            "state": state_tensor,
            "action": action_tensor,
        }

        # Optional outcome annotation
        if self._outcomes:
            # -1, 0, +1 -> float tensor
            outcome_val = float(self._outcomes[idx])
            sample["outcome"] = torch.tensor(outcome_val, dtype=torch.float32)

        return sample

    @property
    def num_features(self) -> int:
        """Number of features in state representation."""
        return self._num_features

    @property
    def num_examples(self) -> int:
        """Number of examples in dataset."""
        return len(self._states)

    @property
    def num_actions(self) -> int:
        """Number of unique actions (vocabulary size)."""
        return self._num_actions

    @property
    def num_invalid_choices(self) -> int:
        """Number of invalid choices that were skipped."""
        return self._num_invalid_choices

    @property
    def num_total_steps(self) -> int:
        """Total number of steps processed (including invalid ones)."""
        return self._num_total_steps

    @property
    def policy_counts(self) -> Dict[str, int]:
        """Example counts per battle_policy_id."""
        return dict(self._policy_counts)

    @property
    def has_outcomes(self) -> bool:
        """True if outcomes are available for all examples."""
        return bool(self._outcomes) and len(self._outcomes) == len(self._states)

    @property
    def outcome_counts(self) -> Dict[int, int]:
        """Counts of outcomes: keys are -1, 0, +1."""
        return dict(self._outcome_counts)


if __name__ == "__main__":
    # Smoke test
    ds = BattleStepDataset(BattleStepDatasetConfig())
    print(
        f"BattleStepDataset: {ds.num_examples} examples, "
        f"{ds.num_features} features, "
        f"{ds.num_actions} unique actions"
    )
    print(f"Num total steps: {ds.num_total_steps}")
    print(f"Num invalid choices skipped: {ds.num_invalid_choices}")
    print(f"Num actions (vocab size): {ds.num_actions}")
    # Optionally show the top 10 most frequent actions
    from collections import Counter

    if ds.num_actions > 0 and ds.action_counts:
        counter = Counter(ds.action_counts)
        top_k = 10
        print(f"Top {top_k} actions:")
        for idx, (action_id, count) in enumerate(
            sorted(counter.items(), key=lambda kv: kv[1], reverse=True)[:top_k],
            start=1,
        ):
            choice_str = ds.id_to_choice[action_id] if 0 <= action_id < len(ds.id_to_choice) else "<unknown>"
            print(f"  #{idx:2d} id={action_id:3d} count={count:5d} choice={choice_str!r}")

    if hasattr(ds, "policy_counts"):
        print(f"Policy counts (battle_policy_id -> examples): {ds.policy_counts}")
    if hasattr(ds, "outcome_counts"):
        print(f"Outcome counts (-1=loss, 0=draw/unknown, +1=win): {ds.outcome_counts}")
        print(f"Has outcomes: {ds.has_outcomes}")

